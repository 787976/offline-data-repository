#flume处理日志数据 
#taildir-kafka-hdfs.conf
#定义agent名字 shyl-applog-agent

#taildir-source的配置
shyl-applog-agent.sources = r1
#指定source 类型（必须）
shyl-applog-agent.sources.r1.type = TAILDIR
#监控的文件组列表（必须）
shyl-applog-agent.sources.r1.filegroups = f1
#文件名 （必须）
shyl-applog-agent.sources.r1.filegroups.f1 = /opt/module/applog/log/app.2023-06-29.log
#记录文件位置的json文件保存路径
shyl-applog-agent.sources.r1.positionFile = /opt/module/flume1.9/shyl-applog-agent/taildir_position.json
#控制从同一文件中连续读取的批数
shyl-applog-agent.sources.r1.maxBatchCount = 1000

#kafka-channel的配置
shyl-applog-agent.channels = c1
#指定channel类型（必须）
shyl-applog-agent.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
#指定kafka集群的地址
shyl-applog-agent.channels.c1.kafka.bootstrap.servers = nodev2001:9092,nodev2002:9092,nodev2003:9092,nodev2004:9092
#指定topic的名称
shyl-applog-agent.channels.c1.kafka.topic = applog
#不以event的形式把数据写入kafka
shyl-applog-agent.channels.c1.parseAsFlumeEvent = false
#指定kafka 消费组
shyl-applog-agent.channels.c1.kafka.consumer.group.id = flume-consumer

#hdfs-sink的配置
shyl-applog-agent.sinks = k1 
#指定sink的类型（必须）
shyl-applog-agent.sinks.k1.type = hdfs
#使用本地时间
shyl-applog-agent.sinks.k1.hdfs.useLocalTimeStamp = true
#指定hdfs 上的路径（必须）
shyl-applog-agent.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d
#文件前缀（先测试）
shyl-applog-agent.sinks.k1.hdfs.filePrefix = test-
#时间戳向下舍
#shyl-applog-agent.sinks.k1.hdfs.round = true
#向下舍入到此值的最高倍数
#shyl-applog-agent.sinks.k1.hdfs.roundValue = 10
#向下舍入值的单位-
#shyl-applog-agent.sinks.k1.hdfs.roundUnit = minute
#时间滚动
shyl-applog-agent.sinks.k1.hdfs.rollInterval = 60
#文件大小滚动(100M)
shyl-applog-agent.sinks.k1.hdfs.rollSize = 104857600
#滚动之前写入文件的事件数 
shyl-applog-agent.sinks.k1.hdfs.rollCount = 0
#source，channel，sink 关联
shyl-applog-agent.sources.r1.channels = c1
shyl-applog-agent.sinks.k1.channel = c1
