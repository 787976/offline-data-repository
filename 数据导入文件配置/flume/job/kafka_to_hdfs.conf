#kafka的日志数据导入hdfs上
# 任务名：kafka-file-hdfs_log
# 定义agent名字 guigu-applog-agent2

#定义组件
guigu-applog-agent2.sources = r1 
guigu-applog-agent2.channels = c1 
guigu-applog-agent2.sinks = k1
# 配置source
guigu-applog-agent2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
guigu-applog-agent2.sources.r1.batchSize = 2000
guigu-applog-agent2.sources.r1.batchDurationMillis = 1000
guigu-applog-agent2.sources.r1.kafka.bootstrap.servers = nodev2001:9092,nodev2002:9092,nodev2003:9092,nodev2004:9092
guigu-applog-agent2.sources.r1.kafka.topics = topic_log
guigu-applog-agent2.sources.r1.kafka.consumer.group.id = topic_log1
#配置拦截器
guigu-applog-agent2.sources.r1.interceptors = i1
guigu-applog-agent2.sources.r1.interceptors.i1.type = com.shyl.interceptor.TimestampInterceptor$Builder

# 配置channel
guigu-applog-agent2.channels.c1.type = file
guigu-applog-agent2.channels.c1.checkpointDir = /opt/module/flume1.9/checkpoint/behavior1
guigu-applog-agent2.channels.c1.dataDirs = /opt/module/flume1.9/data/behavior1

# 配置sink
guigu-applog-agent2.sinks.k1.type = hdfs
guigu-applog-agent2.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/dt=%Y-%m-%d
guigu-applog-agent2.sinks.k1.hdfs.filePrefix = log
#时间滚动
guigu-applog-agent2.sinks.k1.hdfs.rollInterval = 10
#文件大小滚动(100M)
guigu-applog-agent2.sinks.k1.hdfs.rollSize = 104857600
#滚动之前写入文件的事件数 
guigu-applog-agent2.sinks.k1.hdfs.rollCount = 0
#控制输出文件类型
guigu-applog-agent2.sinks.k1.hdfs.fileType = CompressedStream
guigu-applog-agent2.sinks.k1.hdfs.codeC = gzip

# 组装
guigu-applog-agent2.sources.r1.channels = c1
guigu-applog-agent2.sinks.k1.channel = c1 
