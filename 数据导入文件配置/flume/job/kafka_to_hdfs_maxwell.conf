#定义flume job的名字 kafka_to_HDFS_maxwell
#agent的名字：agent3
#定义 source，channel，sink
agent3.sources=r1
agent3.channels=c1
agent3.sinks=k1

#source:kafka

agent3.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
agent3.sources.r1.batchSize = 5000
agent3.sources.r1.batchDurationMillis = 2000
agent3.sources.r1.kafka.bootstrap.servers = nodev2001:9092,nodev2002:9092,nodev2003:9092,nodev2004:9092
agent3.sources.r1.kafka.topics = topic_db
agent3.sources.r1.kafka.consumer.group.id = flume_consumer
#配置拦截器
agent3.sources.r1.interceptors = i1
agent3.sources.r1.interceptors.i1.type = com.shyl.interceptor.TimestampToMSInterceptor$Builder
#channel:file
agent3.channels.c1.type = file
agent3.channels.c1.checkpointDir = /opt/module/flume1.9/checkpoint/behavior2
agent3.channels.c1.dataDirs = /opt/module/flume1.9/data/behavior2

#sink: HDFS
agent3.sinks.k1.type = hdfs
agent3.sinks.k1.hdfs.path = /origin_data/gmall/db/%{table_name}_inc/dt=%Y-%m-%d
agent3.sinks.k1.hdfs.filePrefix = db
agent3.sinks.k1.hdfs.rollInterval=60
agent3.sinks.k1.hdfs.rollSize=125829120
agent3.sinks.k1.hdfs.rollCount = 0
#控制输出文件类型
agent3.sinks.k1.hdfs.fileType = CompressedStream
agent3.sinks.k1.hdfs.codeC = gzip
#组装
agent3.sources.r1.channels =c1
agent3.sinks.k1.channel =c1
